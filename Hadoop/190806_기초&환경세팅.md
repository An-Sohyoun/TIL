190806 - day59 결석으로 빠진 내용 보충 포함

> day60 배운 내용 ① : Hadoop 기초 | Hadoop 설치 | SSH 설정 

## 1. Hadoop 기초

### 1-1. 빅데이터

- 데이터를 수집, 저장, 관리, 분석하는 역량을 넘어 데이터(정형, 비정형) 집합으로부터 가치를 추출하고 결과를 분석하는 기술

- 빅데이터의 3대 요소(3V) + 정확성(Veracity), 가치(Value)

  - 크기(Volume) 

    - 수십 테라바이트(terabyte) 혹은 수십 페타바이트(petabyte) 이상이 해당

    - 데이터 분석을 위해 사용하는 기존 데이터웨어하우스(DW; Data warehouse) 같은 솔루션에서 소화하기 어려울 정도로 급격하게 데이터의 양이 증가하고 있음

    - 이를 극복하기 위하여 확장 가능한 방식으로 데이터를 저장하고 분석하는 분산 컴퓨터 기법 필요

      → 분산 컴퓨팅 솔루션으로 아파치의 **'Hadoop'**을 사용 할 예정

  - 속도(Velocity)

    - 빅데이터는 '실시간 처리'와 '장기적인 접근'이 필요

      > 장기적인 접근이란? 수집된 대량의 데이터를 다양하게 분석하고 표현하기 위함

  - 다양성(Variety)

### 1-2. Hadoop

- 대용량 데이터를 분산 처리 할 수 있는 자바 기반의 오픈소스 프레임워크

- Hadoop은 분산 파일 시스템인 **HDFS(Hadoop Distributed File System)**에 데이터를 저장하고, 

  분산 처리 시스템인 **MapReduce**를 이용하여 데이터를 처리

- 오픈소스이기 때문에 저렴한 구축 비용이 들 뿐 아니라 비용 대비 빠른 데이터 처리

### 1-3. Hadoop 실행 모드

- 독립 실행(Standalone) 모드 : Hadoop의 기본 실행 모드
- 가상 분산(Pseudo-distributed) 모드 : **하나의 장비**에 모든 Hadoop 환경을 설정하는 것
- 완전 분산(Fully distributed) 모드 : **여러 대의 장비**에 Hadoop이 설치된 경우



## 2. Hadoop 설치

> Linux 서버 준비 및 필요 파일(Java, Tomcat, Eclipse, mysql-MariaDB, Hadoop) 다운

### 2-1. Hadoop 설치

- setting에서 network는 'nat'으로 설정하기 → IP 할당받기 위함
- firewall 설정 변경

```
systemctl stop firewalld
systemctl disable firewalld
```

- Hadoop 파일 다운 및 압축 풀기 : [hadoop-1.2.1.tar.gz](https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/[hadoop-1.2.1.tar.gz)

```
tar xvf hadoop-1.2.1.tar.gz → file 디렉토리에 있는 Hadoop 파일 압축 풀기
cp -r hadoop-1.2.1 /etc → 압축 푼 파일을 /etc로 복사
```

- /etc/pforile 파일에 환경변수 등록 → Tomcat 아래에 추가

```
HADOOP_HOME=/etc/hadoop-1.2.1
export JAVA_HOME CLASSPATH TOMCAT_HOME HADOOP_HOME
PATH=.:$JAVA_HOME/bin:$TOMCAT_HOME/bin:$HADOOP_HOME/bin:$PATH
```

- hostname 변경

```
hostnamectl set-hostname hadoopserver1
```

- /etc/hosts : hadoopserver1만 있도록 함

- /etc/sysconfig/network-scripts/ifcfg-ens33 에서 IP 주소 확인

### 2-2. SSH 설정

:computer: SSH(Secure Shell)란? 네트워크 상 다른 컴퓨터에 로그인하거나 원격 시스템에서 명령을 실행할 수 있게 함

- private key와 public key 만듦 (두 개의 키가 쌍으로 생성)

```
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
```

:heavy_check_mark: ​`ls -a`를 통해 '.ssh'가 생성되었는지 확인

:heavy_check_mark: ​id_dsa (private key) / id_dsa.pub (public key)

-  '.ssh'에 접속하여 public key 등록

```
cat id_dsa.pub >> authorized_keys
```

- 비밀번호 없이 로그인이 가능한지 확인

``` 
ssh hadoopserver1 → 바로 'login' 메시지가 떠야 됨 / exit로 로그아웃
```

### 2-3. Hadoop conf 설정

- /etc/hadoop-1.2.1/conf/core-site.xml

> HDFS와 MapReduce에서 공통적으로 사용할 환경정보를 설정

```xml
<configuration>
<property>
  <name>fs.default.name</name>
  <!-- HDFS의 기본 이름을 의미함 / URL 형태로 사용 -->
  <value>hdfs://localhost:9000</value>
</property>
<property>
  <name>hadoop.tmp.dir</name> 
  <!-- 임시 데이터 저장을 위한 공간 -->
  <value>/etc/hadoop-1.2.1/tmp</value>
</property>
</configuration>
```

- /etc/hadoop-1.2.1/conf/hdfs-site.xml

> HDFS에서 사용할 환경 정보를 설정

```xml
<configuration>
<property>
  <name>dfs.replication</name>
  <!-- HDFS의 저장될 데이터의 복제본 개수 -->
  <value>1</value>
</property>
<property>
  <name>dfs.name.dir</name>
  <value>/etc/hadoop-1.2.1/name</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/etc/hadoop-1.2.1/data</value>
</property>
<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>
</configuration>
```

- /etc/hadoop-1.2.1/conf/mapred-site.xml

> MapReduce 환경 정보를 설정

```xml
<configuration>
<property>
  <name>mapred.job.tracker</name>
  <value>localhost:9001</value>
</property>
</configuration>
```

- /etc/hadoop-1.2.1/conf/hadoop-env.sh → 9번 째 줄 수정

```sh
export JAVA_HOME=/etc/jdk1.8
export HADOOP_HOME_WARN_SUPPRESS="TRUE" // 오류메세지 발생 방지
```

- /etc/bashrc → 맨 아래에 추가

```
. /etc/hadoop-1.2.1/conf/hadoop-env.sh
```

> 서버를 켤 때마다 env 실행되도록 설정

- hadoop 포맷 후 `reboot`

```
hadoop namenode -format
```

> /etc/hadoop-1.2.1 아래에 name, data, temp 디렉토리가 생겼으면 제대로 포맷된 것

### 2-4. Hadoop 실행

- hadoop 시작 → `jps` 로 확인

``` 
start-all.sh
```

- hadoop 종료

```
stop-all.sh
```

